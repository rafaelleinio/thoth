{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro\n",
    "\n",
    "This notebook will show how an overview of the library. After running each thoth command you can check the results in the dashboard to better understand the flow and the behavior of the system."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix working dir\n",
    "import pathlib\n",
    "import os\n",
    "\n",
    "path = os.path.join(pathlib.Path().absolute(), \"../\")\n",
    "os.chdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Please set env variable SPARK_VERSION\n"
     ]
    }
   ],
   "source": [
    "# imports\n",
    "from pyspark.sql import SparkSession\n",
    "import pydeequ\n",
    "import json\n",
    "import datetime\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22/11/05 19:11:06 WARN Utils: Your hostname, rleinio-pc resolves to a loopback address: 127.0.1.1; using 192.168.1.132 instead (on interface enp8s0)\n",
      "22/11/05 19:11:06 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/rleinio/.pyenv/versions/3.9.13/envs/thoth-3.9.13/lib/python3.9/site-packages/pyspark/jars/spark-unsafe_2.12-3.0.3.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Ivy Default Cache set to: /home/rleinio/.ivy2/cache\n",
      "The jars for the packages stored in: /home/rleinio/.ivy2/jars\n",
      ":: loading settings :: url = jar:file:/home/rleinio/.pyenv/versions/3.9.13/envs/thoth-3.9.13/lib/python3.9/site-packages/pyspark/jars/ivy-2.4.0.jar!/org/apache/ivy/core/settings/ivysettings.xml\n",
      "com.amazon.deequ#deequ added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-f698148b-05a2-486d-afab-7802abc0d3b2;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.amazon.deequ#deequ;1.2.2-spark-3.0 in central\n",
      "\tfound org.scalanlp#breeze_2.12;0.13.2 in central\n",
      "\tfound org.scalanlp#breeze-macros_2.12;0.13.2 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.1 in central\n",
      "\tfound com.github.fommil.netlib#core;1.1.2 in central\n",
      "\tfound net.sf.opencsv#opencsv;2.3 in central\n",
      "\tfound com.github.rwl#jtransforms;2.4.0 in central\n",
      "\tfound junit#junit;4.8.2 in central\n",
      "\tfound org.apache.commons#commons-math3;3.2 in central\n",
      "\tfound org.spire-math#spire_2.12;0.13.0 in central\n",
      "\tfound org.spire-math#spire-macros_2.12;0.13.0 in central\n",
      "\tfound org.typelevel#machinist_2.12;0.6.1 in central\n",
      "\tfound com.chuusai#shapeless_2.12;2.3.2 in central\n",
      "\tfound org.typelevel#macro-compat_2.12;1.1.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.5 in central\n",
      ":: resolution report :: resolve 305ms :: artifacts dl 6ms\n",
      "\t:: modules in use:\n",
      "\tcom.amazon.deequ#deequ;1.2.2-spark-3.0 from central in [default]\n",
      "\tcom.chuusai#shapeless_2.12;2.3.2 from central in [default]\n",
      "\tcom.github.fommil.netlib#core;1.1.2 from central in [default]\n",
      "\tcom.github.rwl#jtransforms;2.4.0 from central in [default]\n",
      "\tjunit#junit;4.8.2 from central in [default]\n",
      "\tnet.sf.opencsv#opencsv;2.3 from central in [default]\n",
      "\torg.apache.commons#commons-math3;3.2 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.1 from central in [default]\n",
      "\torg.scalanlp#breeze-macros_2.12;0.13.2 from central in [default]\n",
      "\torg.scalanlp#breeze_2.12;0.13.2 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.5 from central in [default]\n",
      "\torg.spire-math#spire-macros_2.12;0.13.0 from central in [default]\n",
      "\torg.spire-math#spire_2.12;0.13.0 from central in [default]\n",
      "\torg.typelevel#machinist_2.12;0.6.1 from central in [default]\n",
      "\torg.typelevel#macro-compat_2.12;1.1.1 from central in [default]\n",
      "\t:: evicted modules:\n",
      "\torg.scala-lang#scala-reflect;2.12.0 by [org.scala-lang#scala-reflect;2.12.1] in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   16  |   0   |   0   |   1   ||   15  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-f698148b-05a2-486d-afab-7802abc0d3b2\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 15 already retrieved (0kB/7ms)\n",
      "22/11/05 19:11:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/11/05 19:11:07 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "source": [
    "# spark context\n",
    "spark = (\n",
    "    SparkSession.builder.config(\"spark.sql.session.timeZone\", \"UTC\")\n",
    "    .config(\"spark.jars.packages\", pydeequ.deequ_maven_coord)\n",
    "    .config(\"spark.jars.excludes\", pydeequ.f2j_maven_coord)\n",
    "    .appName(\"thoth\")\n",
    "    .getOrCreate()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics repository connection\n",
    "os.environ[\"DATABASE_URL\"] = os.environ.get(\n",
    "    \"DATABASE_URL\",\n",
    "    \"postgresql+pg8000://postgres:postgres@localhost:5432/metrics_repository\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset head:  [{'ts': datetime.datetime(1981, 1, 1, 7, 23, 33, tzinfo=datetime.timezone.utc), 'value': 22.1467670458884, 'sensor': 'Sensor E'}, {'ts': datetime.datetime(1981, 1, 1, 21, 57, 57, tzinfo=datetime.timezone.utc), 'value': 22.8849008762327, 'sensor': 'Sensor C'}, {'ts': datetime.datetime(1981, 1, 1, 12, 11, 56, tzinfo=datetime.timezone.utc), 'value': 22.618233805151977, 'sensor': 'Sensor B'}, {'ts': datetime.datetime(1981, 1, 1, 20, 5, 54, tzinfo=datetime.timezone.utc), 'value': 25.770158591638953, 'sensor': 'Sensor E'}, {'ts': datetime.datetime(1981, 1, 1, 10, 45, 2, tzinfo=datetime.timezone.utc), 'value': 23.005804204490918, 'sensor': 'Sensor B'}] \n",
      "\n",
      "Dataset tail:  [{'ts': datetime.datetime(1981, 12, 31, 13, 22, 46, tzinfo=datetime.timezone.utc), 'value': 23.434759603073424, 'sensor': 'Sensor D'}, {'ts': datetime.datetime(1981, 12, 31, 12, 55, 17, tzinfo=datetime.timezone.utc), 'value': 25.313363047160777, 'sensor': 'Sensor B'}, {'ts': datetime.datetime(1981, 12, 31, 3, 4, 55, tzinfo=datetime.timezone.utc), 'value': 23.86284500604152, 'sensor': 'Sensor D'}, {'ts': datetime.datetime(1981, 12, 31, 16, 39, 10, tzinfo=datetime.timezone.utc), 'value': 24.06860268042916, 'sensor': 'Sensor E'}, {'ts': datetime.datetime(1981, 12, 31, 15, 10, 25, tzinfo=datetime.timezone.utc), 'value': 24.52620774970162, 'sensor': 'Sensor C'}] \n",
      "\n",
      "Dataset number of records:  36344 \n",
      "\n",
      "Dataset number of ts daily partitions:  365\n"
     ]
    }
   ],
   "source": [
    "with open(\"sample_datasets/temperatures_extended.json\") as f:\n",
    "    json_data = [\n",
    "        {**record, \"ts\": datetime.datetime.fromisoformat(record.get(\"ts\"))}\n",
    "        for record in json.load(f)\n",
    "    ]\n",
    "print(\"Dataset head: \", json_data[:5], \"\\n\")\n",
    "print(\"Dataset tail: \", json_data[-5:], \"\\n\")\n",
    "print(\"Dataset number of records: \", len(json_data), \"\\n\")\n",
    "print(\n",
    "    \"Dataset number of ts daily partitions: \",\n",
    "    len(set(record.get(\"ts\").date() for record in json_data)),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting dataset into history, new scoring batches, and an artificial anomaly batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# historical data with fair confidence of good quality\n",
    "history_df = spark.createDataFrame(\n",
    "    data=[\n",
    "        record\n",
    "        for record in json_data\n",
    "        if record.get(\"ts\").date() <= datetime.date(1981, 12, 25)\n",
    "    ],\n",
    "    schema=\"ts timestamp, value float, sensor string\",\n",
    ")\n",
    "\n",
    "\n",
    "# new batch of data that need quality validation (normal)\n",
    "new_batch_1981_12_26_df = spark.createDataFrame(\n",
    "    data=[\n",
    "        record\n",
    "        for record in json_data\n",
    "        if record.get(\"ts\").date() == datetime.date(1981, 12, 26)\n",
    "    ],\n",
    "    schema=\"ts timestamp, value float, sensor string\",\n",
    ")\n",
    "\n",
    "\n",
    "# new batch of data that need quality validation (normal)\n",
    "new_batch_1981_12_27_df = spark.createDataFrame(\n",
    "    data=[\n",
    "        record\n",
    "        for record in json_data\n",
    "        if record.get(\"ts\").date() == datetime.date(1981, 12, 27)\n",
    "    ],\n",
    "    schema=\"ts timestamp, value float, sensor string\",\n",
    ")\n",
    "\n",
    "\n",
    "# new batch of data that need quality validation (normal)\n",
    "new_batch_1981_12_28_df = spark.createDataFrame(\n",
    "    data=[\n",
    "        record\n",
    "        for record in json_data\n",
    "        if record.get(\"ts\").date() == datetime.date(1981, 12, 28)\n",
    "    ],\n",
    "    schema=\"ts timestamp, value float, sensor string\",\n",
    ")\n",
    "\n",
    "\n",
    "# new batch of data that need quality validation (normal)\n",
    "new_batch_1981_12_29_df = spark.createDataFrame(\n",
    "    data=[\n",
    "        record\n",
    "        for record in json_data\n",
    "        if record.get(\"ts\").date() == datetime.date(1981, 12, 29)\n",
    "    ],\n",
    "    schema=\"ts timestamp, value float, sensor string\",\n",
    ")\n",
    "\n",
    "\n",
    "# new batch of data that need quality validation (normal)\n",
    "new_batch_1981_12_30_df = spark.createDataFrame(\n",
    "    data=[\n",
    "        record\n",
    "        for record in json_data\n",
    "        if record.get(\"ts\").date() == datetime.date(1981, 12, 30)\n",
    "    ],\n",
    "    schema=\"ts timestamp, value float, sensor string\",\n",
    ")\n",
    "# Artificial anomaly: temperatures in fahrenheit instead of celsius\n",
    "new_batch_1981_12_30_anomaly_df = spark.createDataFrame(\n",
    "    data=[\n",
    "        {\n",
    "            \"ts\": record.get(\"ts\"),\n",
    "            \"value\": ((record.get(\"value\")) * 9 / 5) + 32\n",
    "            if record.get(\"value\")\n",
    "            else None,\n",
    "            \"sensor\": record.get(\"sensor\"),\n",
    "        }\n",
    "        for record in json_data\n",
    "        if record.get(\"ts\").date() == datetime.date(1981, 12, 30)\n",
    "    ],\n",
    "    schema=\"ts timestamp, value float, sensor string\",\n",
    ")\n",
    "# Artificial anomaly: one sensor starts to output only null values\n",
    "new_batch_1981_12_30_anomaly2_df = spark.createDataFrame(\n",
    "    data=[\n",
    "        {\n",
    "            \"ts\": record.get(\"ts\"),\n",
    "            \"value\": None\n",
    "            if record.get(\"sensor\") == \"Sensor B\"\n",
    "            else record.get(\"value\"),\n",
    "            \"sensor\": record.get(\"sensor\"),\n",
    "        }\n",
    "        for record in json_data\n",
    "        if record.get(\"ts\").date() == datetime.date(1981, 12, 30)\n",
    "    ],\n",
    "    schema=\"ts timestamp, value float, sensor string\",\n",
    ")\n",
    "\n",
    "\n",
    "# new batch of data that need quality validation (normal)\n",
    "new_batch_1981_12_31_df = spark.createDataFrame(\n",
    "    data=[\n",
    "        record\n",
    "        for record in json_data\n",
    "        if record.get(\"ts\").date() == datetime.date(1981, 12, 31)\n",
    "    ],\n",
    "    schema=\"ts timestamp, value float, sensor string\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating the Dataset on the Metrics Repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import thoth as th"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup connection and init the Metrics Repository db\n",
    "from sqlmodel import Session\n",
    "\n",
    "session = Session(th.build_engine())\n",
    "th.init_db(clear=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Steps: Profile the history data, create dataset and optimize models for each metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiling, optimization = th.profile_create_optimize(\n",
    "    df=history_df,\n",
    "    dataset_uri=\"temperatures\",\n",
    "    ts_column=\"ts\",\n",
    "    profiling_builder=th.profiler.SimpleProfilingBuilder(),\n",
    "    optimize_last_n=100,\n",
    "    optimize_target_confidence=0.99,\n",
    "    session=session,\n",
    "    spark=spark,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check [this](http://localhost:8501/?dataset_uri=temperatures&view=%F0%9F%91%A4+Profiling) link to open the UI and see the profiling metrics calculated for the `temperatures` dataset. Try also changing the option from `profiling` to `optimization` to check the models validations and which model and threshold were automatically chosen for each profiling time series."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing subsequent new (normal) batches of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.assess_new_ts(\n",
    "    df=new_batch_1981_12_26_df,\n",
    "    ts=datetime.datetime(1981, 12, 26),\n",
    "    dataset_uri=\"temperatures\",\n",
    "    profiling_builder=th.profiler.SimpleProfilingBuilder(),\n",
    "    session=session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.assess_new_ts(\n",
    "    df=new_batch_1981_12_27_df,\n",
    "    ts=datetime.datetime(1981, 12, 27),\n",
    "    dataset_uri=\"temperatures\",\n",
    "    profiling_builder=th.profiler.SimpleProfilingBuilder(),\n",
    "    session=session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.assess_new_ts(\n",
    "    df=new_batch_1981_12_28_df,\n",
    "    ts=datetime.datetime(1981, 12, 28),\n",
    "    dataset_uri=\"temperatures\",\n",
    "    profiling_builder=th.profiler.SimpleProfilingBuilder(),\n",
    "    session=session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.assess_new_ts(\n",
    "    df=new_batch_1981_12_29_df,\n",
    "    ts=datetime.datetime(1981, 12, 29),\n",
    "    dataset_uri=\"temperatures\",\n",
    "    profiling_builder=th.profiler.SimpleProfilingBuilder(),\n",
    "    session=session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the `scoring` option in the dashboard to see these last 4 scorings points, which should be marked as OK 🟢 (normal behavior according to the system)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assessing anomalous batches of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.assess_new_ts(\n",
    "    df=new_batch_1981_12_30_anomaly_df,\n",
    "    ts=datetime.datetime(1981, 12, 30, tzinfo=datetime.timezone.utc),\n",
    "    dataset_uri=\"temperatures\",\n",
    "    profiling_builder=th.profiler.SimpleProfilingBuilder(),\n",
    "    session=session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the `scoring` option in the dashboard to see this last scoring point, which should be marked as Anomaly 🔴"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.assess_new_ts(\n",
    "    df=new_batch_1981_12_30_anomaly2_df,\n",
    "    ts=datetime.datetime(1981, 12, 30, tzinfo=datetime.timezone.utc),\n",
    "    dataset_uri=\"temperatures\",\n",
    "    profiling_builder=th.profiler.SimpleProfilingBuilder(),\n",
    "    session=session,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can check the `scoring` option in the dashboard to see this last scoring point, which should be marked as Anomaly 🔴 again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## After \"fixing/cleaning\" the new batch, continue subsequent assessment of new batches as they arrive at the data platform\n",
    "Finally these next two runs should correct the anomalous batch, and all metrics in the dashboard should come back to an OK 🟢 state again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.assess_new_ts(\n",
    "    df=new_batch_1981_12_30_df,\n",
    "    ts=datetime.datetime(1981, 12, 30, tzinfo=datetime.timezone.utc),\n",
    "    dataset_uri=\"temperatures\",\n",
    "    profiling_builder=th.profiler.SimpleProfilingBuilder(),\n",
    "    session=session,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "th.assess_new_ts(\n",
    "    df=new_batch_1981_12_31_df,\n",
    "    ts=datetime.datetime(1981, 12, 31, tzinfo=datetime.timezone.utc),\n",
    "    dataset_uri=\"temperatures\",\n",
    "    profiling_builder=th.profiler.SimpleProfilingBuilder(),\n",
    "    session=session,\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
